{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3bc24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce161c04",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc45bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "WINDOW = 512\n",
    "OVERLAP = 102\n",
    "SEMANTIC_THRESHOLD = 0.5\n",
    "\n",
    "# Dataset\n",
    "DATASET_PATH = \"data/nq_filtered_medium.jsonl\"\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"avsolatorio/GIST-Embedding-v0\"\n",
    "\n",
    "# Methods to compare\n",
    "CHUNKING_STRATEGIES = [\"sliding_window\", \"html_aware\", \"semantic_similarity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f4dec",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "791aa872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: avsolatorio/GIST-Embedding-v0\n",
      "Model loaded successfully!\n",
      "Max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Max sequence length: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e39dd2",
   "metadata": {},
   "source": [
    "## Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e78e6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_chunk(text, window=512, overlap=102):\n",
    "    \"\"\"Fixed-size sliding window chunking.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    step = window - overlap\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk_words = words[i:i + window]\n",
    "        if not chunk_words:\n",
    "            break\n",
    "        chunks.append(\" \".join(chunk_words))\n",
    "        i += step\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def html_aware_chunk(html_text, max_chunk_size=512):\n",
    "    \"\"\"HTML-structure-aware chunking.\"\"\"\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    chunks = []\n",
    "    structural_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'td', 'div']\n",
    "    \n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "    \n",
    "    def add_chunk():\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    for element in soup.find_all(structural_tags):\n",
    "        text = element.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        if current_word_count + len(words) > max_chunk_size and current_chunk:\n",
    "            add_chunk()\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "        \n",
    "        current_chunk.extend(words)\n",
    "        current_word_count += len(words)\n",
    "        \n",
    "        if element.name in ['h1', 'h2', 'h3'] and current_word_count > max_chunk_size * 0.5:\n",
    "            add_chunk()\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "    \n",
    "    add_chunk()\n",
    "    \n",
    "    # Fallback\n",
    "    if not chunks:\n",
    "        words = html_text.split()\n",
    "        for i in range(0, len(words), max_chunk_size):\n",
    "            chunk_words = words[i:i + max_chunk_size]\n",
    "            chunks.append(\" \".join(chunk_words))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def semantic_similarity_chunk(text, model, threshold=0.5, max_chunk_size=512):\n",
    "    \"\"\"Semantic similarity-based chunking.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) <= 1:\n",
    "        return [text]\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(sentence_embeddings) - 1):\n",
    "        sim = util.cos_sim(sentence_embeddings[i], sentence_embeddings[i + 1])[0][0].item()\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_word_count = len(sentences[0].split())\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        next_sentence = sentences[i + 1]\n",
    "        next_word_count = len(next_sentence.split())\n",
    "        \n",
    "        if sim < threshold or (current_word_count + next_word_count > max_chunk_size):\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [next_sentence]\n",
    "            current_word_count = next_word_count\n",
    "        else:\n",
    "            current_chunk.append(next_sentence)\n",
    "            current_word_count += next_word_count\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73433f6",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ce817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gold_chunk_by_content(chunks, document_tokens, start_token, end_token):\n",
    "    \"\"\"Find which chunk contains the gold answer.\"\"\"\n",
    "    if start_token < 0 or end_token < 0 or start_token >= len(document_tokens):\n",
    "        return None\n",
    "    \n",
    "    gold_tokens = [document_tokens[i]['token'] for i in range(start_token, min(end_token, len(document_tokens)))]\n",
    "    gold_text = \" \".join(gold_tokens).lower()\n",
    "    \n",
    "    # Exact match\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if gold_text in chunk.lower():\n",
    "            return idx\n",
    "    \n",
    "    # Fuzzy match\n",
    "    gold_words = set(gold_text.split())\n",
    "    best_idx = None\n",
    "    best_overlap = 0\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        overlap = len(gold_words & chunk_words)\n",
    "        if overlap > best_overlap:\n",
    "            best_overlap = overlap\n",
    "            best_idx = idx\n",
    "    \n",
    "    if best_overlap >= len(gold_words) * 0.5:\n",
    "        return best_idx\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_metrics(rank_list):\n",
    "    \"\"\"Compute Recall@10 and MRR.\"\"\"\n",
    "    recall10 = np.mean([1 if r <= 10 else 0 for r in rank_list])\n",
    "    mrr = np.mean([1.0 / r for r in rank_list])\n",
    "    return recall10, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b70f1b",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fcfe751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(model, dataset_path, chunking_strategy, method_name, **chunk_params):\n",
    "    \"\"\"\n",
    "    Evaluate a chunking strategy with naive encoding.\n",
    "    \n",
    "    Args:\n",
    "        model: SentenceTransformer model\n",
    "        dataset_path: Path to dataset\n",
    "        chunking_strategy: Function that returns chunk texts\n",
    "        method_name: Name for logging\n",
    "        **chunk_params: Parameters for chunking function\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics\n",
    "    \"\"\"\n",
    "    rank_list = []\n",
    "    skipped = 0\n",
    "    \n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Evaluating {method_name}\"):\n",
    "            item = json.loads(line)\n",
    "            \n",
    "            question = item[\"question_text\"]\n",
    "            html_text = item[\"document_html\"]\n",
    "            doc_tokens = item[\"document_tokens\"]\n",
    "            \n",
    "            # Apply chunking strategy\n",
    "            try:\n",
    "                if \"semantic\" in method_name:\n",
    "                    chunks = chunking_strategy(html_text, model, **chunk_params)\n",
    "                else:\n",
    "                    chunks = chunking_strategy(html_text, **chunk_params)\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            if not chunks:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Encode chunks (naive encoding: each chunk independently)\n",
    "            try:\n",
    "                chunk_embeddings = model.encode(chunks, convert_to_tensor=True, show_progress_bar=False)\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Encode query\n",
    "            query_embedding = model.encode(question, convert_to_tensor=True, show_progress_bar=False)\n",
    "            \n",
    "            # Similarity ranking\n",
    "            scores = util.cos_sim(query_embedding, chunk_embeddings)[0]\n",
    "            ranking = scores.argsort(descending=True).cpu().numpy()\n",
    "            \n",
    "            # Find gold answer\n",
    "            ann = item[\"annotations\"][0]\n",
    "            if ann[\"short_answers\"]:\n",
    "                gold_start = ann[\"short_answers\"][0][\"start_token\"]\n",
    "                gold_end = ann[\"short_answers\"][0][\"end_token\"]\n",
    "            else:\n",
    "                gold_start = ann[\"long_answer\"][\"start_token\"]\n",
    "                gold_end = ann[\"long_answer\"][\"end_token\"]\n",
    "            \n",
    "            if gold_start < 0 or gold_end < 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            gold_chunk = find_gold_chunk_by_content(chunks, doc_tokens, gold_start, gold_end)\n",
    "            if gold_chunk is None or gold_chunk >= len(chunks):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Find rank\n",
    "            gold_rank = np.where(ranking == gold_chunk)[0][0] + 1\n",
    "            rank_list.append(gold_rank)\n",
    "    \n",
    "    # Compute metrics\n",
    "    if rank_list:\n",
    "        recall10, mrr = compute_metrics(rank_list)\n",
    "    else:\n",
    "        recall10, mrr = 0.0, 0.0\n",
    "    \n",
    "    return {\n",
    "        \"method\": method_name,\n",
    "        \"recall@10\": recall10,\n",
    "        \"mrr\": mrr,\n",
    "        \"total_samples\": len(rank_list),\n",
    "        \"skipped\": skipped\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652c2a1",
   "metadata": {},
   "source": [
    "## Run All Experiments\n",
    "\n",
    "Compare the three chunking strategies: Sliding Window, HTML-Aware, and Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Running Chunking Strategy Comparison\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Chunking Strategy: SLIDING_WINDOW\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sliding_window: 25it [00:09,  2.57it/s]"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Running Chunking Strategy Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "strategies_map = {\n",
    "    \"sliding_window\": (sliding_window_chunk, {\"window\": WINDOW, \"overlap\": OVERLAP}),\n",
    "    \"html_aware\": (html_aware_chunk, {\"max_chunk_size\": WINDOW}),\n",
    "    \"semantic_similarity\": (semantic_similarity_chunk, {\"threshold\": SEMANTIC_THRESHOLD, \"max_chunk_size\": WINDOW})\n",
    "}\n",
    "\n",
    "for strategy_name, (strategy_func, params) in strategies_map.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Chunking Strategy: {strategy_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results[strategy_name] = evaluate_method(\n",
    "        model=model,\n",
    "        dataset_path=DATASET_PATH,\n",
    "        chunking_strategy=strategy_func,\n",
    "        method_name=strategy_name,\n",
    "        **params\n",
    "    )\n",
    "    print(f\"  Recall@10: {results[strategy_name]['recall@10']:.4f}, MRR: {results[strategy_name]['mrr']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"All experiments completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bdbb6",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f13b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results).T\n",
    "\n",
    "print(\"\\n=== Chunking Strategy Comparison Results ===\")\n",
    "print(df_results.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"chunking_comparison_results.csv\")\n",
    "print(\"\\nResults saved to chunking_comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de92a0",
   "metadata": {},
   "source": [
    "## Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by Recall@10\n",
    "sorted_by_recall = sorted(results.items(), key=lambda x: x[1]['recall@10'], reverse=True)\n",
    "\n",
    "print(\"\\nRanking by Recall@10:\")\n",
    "for i, (strategy, metrics) in enumerate(sorted_by_recall, 1):\n",
    "    print(f\"  {i}. {strategy.upper().replace('_', ' ')}: Recall@10={metrics['recall@10']:.4f}, MRR={metrics['mrr']:.4f}\")\n",
    "\n",
    "# Best vs Worst comparison\n",
    "best_strategy, best_metrics = sorted_by_recall[0]\n",
    "worst_strategy, worst_metrics = sorted_by_recall[-1]\n",
    "\n",
    "recall_improvement = ((best_metrics['recall@10'] - worst_metrics['recall@10']) / worst_metrics['recall@10'] * 100) if worst_metrics['recall@10'] > 0 else 0\n",
    "mrr_improvement = ((best_metrics['mrr'] - worst_metrics['mrr']) / worst_metrics['mrr'] * 100) if worst_metrics['mrr'] > 0 else 0\n",
    "\n",
    "print(f\"\\nüèÜ Best Strategy: {best_strategy.upper().replace('_', ' ')}\")\n",
    "print(f\"   Recall@10: {best_metrics['recall@10']:.4f}\")\n",
    "print(f\"   MRR: {best_metrics['mrr']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Improvement over worst strategy:\")\n",
    "print(f\"   Recall@10: {recall_improvement:+.2f}%\")\n",
    "print(f\"   MRR: {mrr_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736608cf",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13380f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "strategies = [\"Sliding Window\", \"HTML-Aware\", \"Semantic Similarity\"]\n",
    "recalls = [results[\"sliding_window\"]['recall@10'], \n",
    "           results[\"html_aware\"]['recall@10'], \n",
    "           results[\"semantic_similarity\"]['recall@10']]\n",
    "mrrs = [results[\"sliding_window\"]['mrr'], \n",
    "        results[\"html_aware\"]['mrr'], \n",
    "        results[\"semantic_similarity\"]['mrr']]\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.6\n",
    "\n",
    "# Define colors for each strategy\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Plot 1: Recall@10 Comparison\n",
    "bars1 = axes[0].bar(x, recalls, width, color=colors)\n",
    "\n",
    "axes[0].set_ylabel('Recall@10', fontsize=12)\n",
    "axes[0].set_title('Recall@10 Comparison Across Chunking Strategies', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(strategies)\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: MRR Comparison\n",
    "bars2 = axes[1].bar(x, mrrs, width, color=colors)\n",
    "\n",
    "axes[1].set_ylabel('MRR', fontsize=12)\n",
    "axes[1].set_title('MRR Comparison Across Chunking Strategies', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(strategies)\n",
    "axes[1].set_ylim([0, max(mrrs) * 1.2])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chunking_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to chunking_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ff0b1",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best method\n",
    "best_method = max(results.items(), key=lambda x: x[1]['recall@10'])\n",
    "print(f\"\\nüèÜ Best Chunking Strategy: {best_method[0].upper().replace('_', ' ')}\")\n",
    "print(f\"   Recall@10: {best_method[1]['recall@10']:.4f}\")\n",
    "print(f\"   MRR: {best_method[1]['mrr']:.4f}\")\n",
    "print(f\"   Samples Evaluated: {best_method[1]['total_samples']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis experiment compares three chunking strategies for document retrieval:\")\n",
    "print(\"\\n1. Sliding Window: Fixed-size chunks with overlap\")\n",
    "print(\"   - Simple and fast\")\n",
    "print(\"   - May split semantic units\")\n",
    "print(\"\\n2. HTML-Aware: Respects HTML structure (headers, paragraphs)\")\n",
    "print(\"   - Preserves document structure\")\n",
    "print(\"   - Good for HTML documents\")\n",
    "print(\"\\n3. Semantic Similarity: Groups sentences by semantic similarity\")\n",
    "print(\"   - Preserves semantic coherence\")\n",
    "print(\"   - More computationally expensive\")\n",
    "\n",
    "print(f\"\\nBest performing strategy: {best_method[0].upper().replace('_', ' ')}\")\n",
    "print(\"\\nAll chunks are encoded using naive encoding (each chunk independently).\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
