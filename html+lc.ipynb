{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSUqa6orONNx",
        "outputId": "0f06247d-1c89-45c3-eaf4-8cf357f0876d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "HF_BACKBONE = \"jinaai/jina-embeddings-v2-base-en\"\n",
        "\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(HF_BACKBONE)\n",
        "hf_model = AutoModel.from_pretrained(HF_BACKBONE).to(device)\n",
        "\n",
        "# ST model for fallback + query encoding\n",
        "st_model = SentenceTransformer(HF_BACKBONE, device=device)\n",
        "\n",
        "\n",
        "def encode_full_doc_tokens(text, max_len=512):\n",
        "    # First tokenize WITHOUT tensors so we can manually segment\n",
        "    base = hf_tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "    ids = base[\"input_ids\"]\n",
        "    offsets = base[\"offset_mapping\"]\n",
        "\n",
        "    all_emb = []\n",
        "    all_offsets = []\n",
        "\n",
        "    # Split long text into segments of <=512 tokens\n",
        "    for i in range(0, len(ids), max_len):\n",
        "        seg_ids = ids[i:i+max_len]\n",
        "        seg_offsets = offsets[i:i+max_len]\n",
        "\n",
        "        seg_inputs = {\n",
        "            \"input_ids\": torch.tensor([seg_ids], dtype=torch.long, device=device),\n",
        "            \"attention_mask\": torch.ones((1, len(seg_ids)), dtype=torch.long, device=device)\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = hf_model(**seg_inputs)\n",
        "\n",
        "        hidden = out.last_hidden_state.squeeze(0)   # shape = (seg_len, hidden_dim)\n",
        "        all_emb.append(hidden.cpu())\n",
        "        all_offsets.extend(seg_offsets)\n",
        "\n",
        "    full_emb = torch.cat(all_emb, dim=0).to(device)\n",
        "    return full_emb, all_offsets\n",
        "\n",
        "\n",
        "\n",
        "def find_token_span_for_chunk(chunk_text, full_text, offsets):\n",
        "    start_char = full_text.find(chunk_text)\n",
        "    if start_char == -1:\n",
        "        return None, None\n",
        "\n",
        "    end_char = start_char + len(chunk_text)\n",
        "\n",
        "    start_tok = None\n",
        "    end_tok = None\n",
        "\n",
        "    for i, (c_start, c_end) in enumerate(offsets):\n",
        "        if c_start <= start_char < c_end:\n",
        "            start_tok = i\n",
        "        if c_start < end_char <= c_end:\n",
        "            end_tok = i\n",
        "\n",
        "    if start_tok is None:\n",
        "        return None, None\n",
        "    if end_tok is None:\n",
        "        end_tok = start_tok\n",
        "\n",
        "    return start_tok, end_tok\n",
        "\n",
        "\n",
        "\n",
        "def pool_chunk(token_emb, start_tok, end_tok):\n",
        "    sub = token_emb[start_tok:end_tok+1]\n",
        "    return sub.mean(dim=0)\n",
        "\n",
        "\n",
        "\n",
        "def late_chunking_encode(html_text, chunks):\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    full_text = soup.get_text(\" \", strip=True)\n",
        "\n",
        "    token_emb, offsets = encode_full_doc_tokens(full_text)\n",
        "\n",
        "    out = []\n",
        "\n",
        "    for ch in chunks:\n",
        "        st, ed = find_token_span_for_chunk(ch, full_text, offsets)\n",
        "        if st is None:\n",
        "            # fallback\n",
        "            out.append(st_model.encode(ch, convert_to_tensor=True))\n",
        "            continue\n",
        "\n",
        "        pooled = pool_chunk(token_emb, st, ed)\n",
        "        out.append(pooled)\n",
        "\n",
        "    return torch.stack(out).to(device)\n",
        "\n",
        "\n",
        "def html_aware_chunk(html_text, max_chunk_size=512):\n",
        "    \"\"\"HTML-structure-aware chunking.\"\"\"\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    chunks = []\n",
        "    structural_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'td', 'div']\n",
        "\n",
        "    current_chunk = []\n",
        "    current_word_count = 0\n",
        "\n",
        "    def add_chunk():\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    for element in soup.find_all(structural_tags):\n",
        "        text = element.get_text(strip=True)\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        words = text.split()\n",
        "\n",
        "        if current_word_count + len(words) > max_chunk_size and current_chunk:\n",
        "            add_chunk()\n",
        "            current_chunk = []\n",
        "            current_word_count = 0\n",
        "\n",
        "        current_chunk.extend(words)\n",
        "        current_word_count += len(words)\n",
        "\n",
        "        if element.name in ['h1', 'h2', 'h3'] and current_word_count > max_chunk_size * 0.5:\n",
        "            add_chunk()\n",
        "            current_chunk = []\n",
        "            current_word_count = 0\n",
        "\n",
        "    add_chunk()\n",
        "\n",
        "    # Fallback\n",
        "    if not chunks:\n",
        "        words = html_text.split()\n",
        "        for i in range(0, len(words), max_chunk_size):\n",
        "            chunk_words = words[i:i + max_chunk_size]\n",
        "            chunks.append(\" \".join(chunk_words))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "def find_gold_chunk(chunks, document_tokens, start_tok, end_tok):\n",
        "    # convert official tokens to string\n",
        "    gold = \" \".join([document_tokens[i][\"token\"].lower()\n",
        "                     for i in range(start_tok, end_tok)\n",
        "                     if i < len(document_tokens)])\n",
        "    if not gold:\n",
        "        return None\n",
        "\n",
        "    # exact search\n",
        "    for i, ch in enumerate(chunks):\n",
        "        if gold in ch.lower():\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def evaluate(dataset_path):\n",
        "    t0 = time.time()\n",
        "\n",
        "    ranks = []\n",
        "    skipped = 0\n",
        "    total_chunks = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    tqdm_bar = tqdm(desc=f\"Evaluating {method_name}\")\n",
        "\n",
        "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for item in map(json.loads, f):\n",
        "            tqdm_bar.update(1)\n",
        "            total_samples += 1\n",
        "\n",
        "            html = item[\"document_html\"]\n",
        "            question = item[\"question_text\"]\n",
        "            doc_tokens = item[\"document_tokens\"]\n",
        "\n",
        "            chunks = html_aware_chunk(html, max_chunk_size=512)\n",
        "            if not chunks:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            total_chunks += len(chunks)\n",
        "            try:\n",
        "                chunk_emb = late_chunking_encode(html, chunks)\n",
        "                q_emb = st_model.encode(question, convert_to_tensor=True)\n",
        "            except Exception as e:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            scores = util.cos_sim(q_emb, chunk_emb)[0]\n",
        "            ranking = scores.argsort(descending=True).cpu().numpy()\n",
        "\n",
        "            ann = item[\"annotations\"][0]\n",
        "            if ann[\"short_answers\"]:\n",
        "                gs = ann[\"short_answers\"][0][\"start_token\"]\n",
        "                ge = ann[\"short_answers\"][0][\"end_token\"]\n",
        "            else:\n",
        "                gs = ann[\"long_answer\"][\"start_token\"]\n",
        "                ge = ann[\"long_answer\"][\"end_token\"]\n",
        "\n",
        "            if gs < 0 or ge < 0:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            gold_idx = find_gold_chunk(chunks, doc_tokens, gs, ge)\n",
        "            if gold_idx is None:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            rank = np.where(ranking == gold_idx)[0][0] + 1\n",
        "            ranks.append(rank)\n",
        "\n",
        "    tqdm_time_str = tqdm_bar.format_dict[\"elapsed\"]\n",
        "    tqdm_bar.close()\n",
        "\n",
        "    recall10 = np.mean([1 if r <= 10 else 0 for r in ranks]) if ranks else 0\n",
        "    mrr = np.mean([1.0 / r for r in ranks]) if ranks else 0\n",
        "    ndcg = np.mean([1 / np.log2(r + 1) for r in ranks]) if ranks else 0\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    items_per_sec = total_samples / elapsed if elapsed > 0 else 0\n",
        "    avg_num_chunks = total_chunks / max(len(ranks), 1)\n",
        "\n",
        "    result = {\n",
        "        \"method\": method_name,\n",
        "        \"recall@10\": float(recall10),\n",
        "        \"mrr\": float(mrr),\n",
        "        \"ndcg\": float(ndcg),\n",
        "        \"total_samples\": total_samples,\n",
        "        \"skipped\": skipped,\n",
        "        \"avg_num_chunks\": float(avg_num_chunks),\n",
        "        \"elapsed_seconds\": float(elapsed),\n",
        "        \"items_per_sec\": float(items_per_sec),\n",
        "        \"tqdm_time_str\": str(tqdm_time_str),\n",
        "        \"dataset\": dataset_path,\n",
        "    }\n",
        "\n",
        "    print(json.dumps(result, indent=2))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej-EMksoOPdv",
        "outputId": "c7632b13-4584-46d9-9a2c-32205e2166b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w3KYskPOPWd",
        "outputId": "ee3563b3-810d-4691-ce10-d8f4d51951c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CIS-5200-final-Text2Vec\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CIS-5200-final-Text2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff9M1Sm5kF4n"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(\"result\", exist_ok=True)\n",
        "  # short\n",
        "    with open(\"result/short_results_html+lc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        sys.stdout = f\n",
        "        evaluate(\"data/nq_dev_short_cleaned.jsonl\")\n",
        "\n",
        "    # medium\n",
        "    with open(\"result/medium_results_html+lc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        sys.stdout = f\n",
        "        evaluate(\"data/nq_dev_medium_cleaned.jsonl\")\n",
        "\n",
        "    # long\n",
        "    with open(\"result/long_results_html+lc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        sys.stdout = f\n",
        "        evaluate(\"data/nq_dev_long_cleaned.jsonl\")\n",
        "\n",
        "    sys.stdout = sys.__stdout__\n",
        "    print(\"Done! Results saved to result folder.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
