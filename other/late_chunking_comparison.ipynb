{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6a3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cemma/Downloads/CIS5200_FinalProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/cemma/Downloads/CIS5200_FinalProject/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c44a85",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b631c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "WINDOW = 512\n",
    "OVERLAP = 102\n",
    "SEMANTIC_THRESHOLD = 0.5\n",
    "MAX_TOKENS = 8192  # Maximum context length for the model\n",
    "\n",
    "# Dataset\n",
    "DATASET_PATH = \"data/nq_filtered_medium.jsonl\"\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"avsolatorio/GIST-Embedding-v0\"\n",
    "\n",
    "# Methods to compare\n",
    "CHUNKING_STRATEGIES = [\"sliding_window\", \"html_aware\", \"semantic_similarity\"]\n",
    "ENCODING_METHODS = [\"naive\", \"late_chunking\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2ff94",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b06049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: avsolatorio/GIST-Embedding-v0\n",
      "Model and tokenizer loaded successfully!\n",
      "Max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "# Get the underlying transformer and tokenizer\n",
    "transformer = model[0].auto_model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "print(f\"Max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf9761",
   "metadata": {},
   "source": [
    "## Chunking Strategies (Same as Before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee827f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_chunk(text, window=512, overlap=102):\n",
    "    \"\"\"Fixed-size sliding window chunking.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    step = window - overlap\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk_words = words[i:i + window]\n",
    "        if not chunk_words:\n",
    "            break\n",
    "        chunks.append(\" \".join(chunk_words))\n",
    "        i += step\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def html_aware_chunk(html_text, max_chunk_size=512):\n",
    "    \"\"\"HTML-structure-aware chunking.\"\"\"\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    chunks = []\n",
    "    structural_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'td', 'div']\n",
    "    \n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "    \n",
    "    def add_chunk():\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    for element in soup.find_all(structural_tags):\n",
    "        text = element.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        if current_word_count + len(words) > max_chunk_size and current_chunk:\n",
    "            add_chunk()\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "        \n",
    "        current_chunk.extend(words)\n",
    "        current_word_count += len(words)\n",
    "        \n",
    "        if element.name in ['h1', 'h2', 'h3'] and current_word_count > max_chunk_size * 0.5:\n",
    "            add_chunk()\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "    \n",
    "    add_chunk()\n",
    "    \n",
    "    # Fallback\n",
    "    if not chunks:\n",
    "        words = html_text.split()\n",
    "        for i in range(0, len(words), max_chunk_size):\n",
    "            chunk_words = words[i:i + max_chunk_size]\n",
    "            chunks.append(\" \".join(chunk_words))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def semantic_similarity_chunk(text, model, threshold=0.5, max_chunk_size=512):\n",
    "    \"\"\"Semantic similarity-based chunking.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) <= 1:\n",
    "        return [text]\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(sentence_embeddings) - 1):\n",
    "        sim = util.cos_sim(sentence_embeddings[i], sentence_embeddings[i + 1])[0][0].item()\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_word_count = len(sentences[0].split())\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        next_sentence = sentences[i + 1]\n",
    "        next_word_count = len(next_sentence.split())\n",
    "        \n",
    "        if sim < threshold or (current_word_count + next_word_count > max_chunk_size):\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [next_sentence]\n",
    "            current_word_count = next_word_count\n",
    "        else:\n",
    "            current_chunk.append(next_sentence)\n",
    "            current_word_count += next_word_count\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a4b0e",
   "metadata": {},
   "source": [
    "## Late Chunking Implementation\n",
    "\n",
    "The key innovation: encode entire document first, then apply mean pooling to token embeddings at chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c05168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking_encode(text, chunk_texts, model, tokenizer, max_length=8192):\n",
    "    \"\"\"\n",
    "    Late Chunking: Encode entire document, then chunk at token level.\n",
    "    \n",
    "    Args:\n",
    "        text: Full document text\n",
    "        chunk_texts: List of chunk texts (defines boundaries)\n",
    "        model: SentenceTransformer model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        List of chunk embeddings (tensors)\n",
    "    \"\"\"\n",
    "    # Tokenize the entire document\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get token embeddings from the transformer\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state[0]  # Shape: (num_tokens, hidden_dim)\n",
    "    \n",
    "    # Now find token ranges for each chunk\n",
    "    chunk_embeddings = []\n",
    "    current_char_pos = 0\n",
    "    \n",
    "    for chunk_text in chunk_texts:\n",
    "        # Find where this chunk appears in the original text\n",
    "        chunk_start_char = text.find(chunk_text, current_char_pos)\n",
    "        \n",
    "        if chunk_start_char == -1:\n",
    "            # Fallback: encode chunk independently if not found\n",
    "            chunk_emb = model.encode(chunk_text, convert_to_tensor=True, show_progress_bar=False)\n",
    "            chunk_embeddings.append(chunk_emb)\n",
    "            continue\n",
    "        \n",
    "        chunk_end_char = chunk_start_char + len(chunk_text)\n",
    "        \n",
    "        # Convert character positions to token positions\n",
    "        # This is approximate - we find tokens that overlap with the chunk\n",
    "        chunk_token_ids = []\n",
    "        char_to_token = inputs.char_to_token(0, chunk_start_char)\n",
    "        \n",
    "        if char_to_token is None:\n",
    "            # Fallback if mapping fails\n",
    "            chunk_emb = model.encode(chunk_text, convert_to_tensor=True, show_progress_bar=False)\n",
    "            chunk_embeddings.append(chunk_emb)\n",
    "            current_char_pos = chunk_end_char\n",
    "            continue\n",
    "        \n",
    "        start_token = char_to_token\n",
    "        \n",
    "        # Find end token\n",
    "        end_token = start_token\n",
    "        for char_pos in range(chunk_start_char, min(chunk_end_char, len(text))):\n",
    "            token_idx = inputs.char_to_token(0, char_pos)\n",
    "            if token_idx is not None:\n",
    "                end_token = max(end_token, token_idx)\n",
    "        \n",
    "        end_token = min(end_token + 1, len(token_embeddings))  # +1 for exclusive end\n",
    "        \n",
    "        # Mean pooling over the token embeddings for this chunk\n",
    "        if start_token < end_token:\n",
    "            chunk_token_embs = token_embeddings[start_token:end_token]\n",
    "            chunk_emb = torch.mean(chunk_token_embs, dim=0)\n",
    "        else:\n",
    "            # Fallback\n",
    "            chunk_emb = model.encode(chunk_text, convert_to_tensor=True, show_progress_bar=False)\n",
    "        \n",
    "        chunk_embeddings.append(chunk_emb)\n",
    "        current_char_pos = chunk_end_char\n",
    "    \n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3c892",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d369d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gold_chunk_by_content(chunks, document_tokens, start_token, end_token):\n",
    "    \"\"\"Find which chunk contains the gold answer.\"\"\"\n",
    "    if start_token < 0 or end_token < 0 or start_token >= len(document_tokens):\n",
    "        return None\n",
    "    \n",
    "    gold_tokens = [document_tokens[i]['token'] for i in range(start_token, min(end_token, len(document_tokens)))]\n",
    "    gold_text = \" \".join(gold_tokens).lower()\n",
    "    \n",
    "    # Exact match\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if gold_text in chunk.lower():\n",
    "            return idx\n",
    "    \n",
    "    # Fuzzy match\n",
    "    gold_words = set(gold_text.split())\n",
    "    best_idx = None\n",
    "    best_overlap = 0\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        overlap = len(gold_words & chunk_words)\n",
    "        if overlap > best_overlap:\n",
    "            best_overlap = overlap\n",
    "            best_idx = idx\n",
    "    \n",
    "    if best_overlap >= len(gold_words) * 0.5:\n",
    "        return best_idx\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_metrics(rank_list):\n",
    "    \"\"\"Compute Recall@10 and MRR.\"\"\"\n",
    "    recall10 = np.mean([1 if r <= 10 else 0 for r in rank_list])\n",
    "    mrr = np.mean([1.0 / r for r in rank_list])\n",
    "    return recall10, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e65d2",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "Supports both naive and late chunking encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23650f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(model, tokenizer, dataset_path, chunking_strategy, encoding_method, method_name, **chunk_params):\n",
    "    \"\"\"\n",
    "    Evaluate a combination of chunking strategy and encoding method.\n",
    "    \n",
    "    Args:\n",
    "        model: SentenceTransformer model\n",
    "        tokenizer: Tokenizer\n",
    "        dataset_path: Path to dataset\n",
    "        chunking_strategy: Function that returns chunk texts\n",
    "        encoding_method: 'naive' or 'late_chunking'\n",
    "        method_name: Name for logging\n",
    "        **chunk_params: Parameters for chunking function\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics\n",
    "    \"\"\"\n",
    "    rank_list = []\n",
    "    skipped = 0\n",
    "    \n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Evaluating {method_name}\"):\n",
    "            item = json.loads(line)\n",
    "            \n",
    "            question = item[\"question_text\"]\n",
    "            html_text = item[\"document_html\"]\n",
    "            doc_tokens = item[\"document_tokens\"]\n",
    "            \n",
    "            # Apply chunking strategy\n",
    "            try:\n",
    "                if \"semantic\" in method_name:\n",
    "                    chunks = chunking_strategy(html_text, model, **chunk_params)\n",
    "                else:\n",
    "                    chunks = chunking_strategy(html_text, **chunk_params)\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            if not chunks:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Encode chunks based on method\n",
    "            try:\n",
    "                if encoding_method == \"naive\":\n",
    "                    # Naive: encode each chunk independently\n",
    "                    chunk_embeddings = model.encode(chunks, convert_to_tensor=True, show_progress_bar=False)\n",
    "                else:\n",
    "                    # Late chunking: encode full document first\n",
    "                    chunk_embeddings = late_chunking_encode(html_text, chunks, model, tokenizer, max_length=MAX_TOKENS)\n",
    "                    chunk_embeddings = torch.stack(chunk_embeddings)\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Encode query\n",
    "            query_embedding = model.encode(question, convert_to_tensor=True, show_progress_bar=False)\n",
    "            \n",
    "            # Similarity ranking\n",
    "            scores = util.cos_sim(query_embedding, chunk_embeddings)[0]\n",
    "            ranking = scores.argsort(descending=True).cpu().numpy()\n",
    "            \n",
    "            # Find gold answer\n",
    "            ann = item[\"annotations\"][0]\n",
    "            if ann[\"short_answers\"]:\n",
    "                gold_start = ann[\"short_answers\"][0][\"start_token\"]\n",
    "                gold_end = ann[\"short_answers\"][0][\"end_token\"]\n",
    "            else:\n",
    "                gold_start = ann[\"long_answer\"][\"start_token\"]\n",
    "                gold_end = ann[\"long_answer\"][\"end_token\"]\n",
    "            \n",
    "            if gold_start < 0 or gold_end < 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            gold_chunk = find_gold_chunk_by_content(chunks, doc_tokens, gold_start, gold_end)\n",
    "            if gold_chunk is None or gold_chunk >= len(chunks):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Find rank\n",
    "            gold_rank = np.where(ranking == gold_chunk)[0][0] + 1\n",
    "            rank_list.append(gold_rank)\n",
    "    \n",
    "    # Compute metrics\n",
    "    if rank_list:\n",
    "        recall10, mrr = compute_metrics(rank_list)\n",
    "    else:\n",
    "        recall10, mrr = 0.0, 0.0\n",
    "    \n",
    "    return {\n",
    "        \"method\": method_name,\n",
    "        \"recall@10\": recall10,\n",
    "        \"mrr\": mrr,\n",
    "        \"total_samples\": len(rank_list),\n",
    "        \"skipped\": skipped\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd1eb1",
   "metadata": {},
   "source": [
    "## Run All Experiments\n",
    "\n",
    "Compare naive vs late chunking for each chunking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6770f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Running Late Chunking vs Naive Encoding Comparison\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Chunking Strategy: SLIDING_WINDOW\n",
      "======================================================================\n",
      "\n",
      "  1. Naive Encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sliding_window_naive: 80it [00:34,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Recall@10: 0.5556, MRR: 0.2243\n",
      "\n",
      "  2. Late Chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sliding_window_late: 80it [00:05, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Recall@10: 0.0000, MRR: 0.0000\n",
      "\n",
      "======================================================================\n",
      "Chunking Strategy: HTML_AWARE\n",
      "======================================================================\n",
      "\n",
      "  1. Naive Encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating html_aware_naive: 45it [00:13,  3.00it/s]"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Running Late Chunking vs Naive Encoding Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "strategies_map = {\n",
    "    \"sliding_window\": (sliding_window_chunk, {\"window\": WINDOW, \"overlap\": OVERLAP}),\n",
    "    \"html_aware\": (html_aware_chunk, {\"max_chunk_size\": WINDOW}),\n",
    "    \"semantic_similarity\": (semantic_similarity_chunk, {\"threshold\": SEMANTIC_THRESHOLD, \"max_chunk_size\": WINDOW})\n",
    "}\n",
    "\n",
    "for strategy_name, (strategy_func, params) in strategies_map.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Chunking Strategy: {strategy_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Naive encoding\n",
    "    print(f\"\\n  1. Naive Encoding...\")\n",
    "    method_name = f\"{strategy_name}_naive\"\n",
    "    results[method_name] = evaluate_method(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_path=DATASET_PATH,\n",
    "        chunking_strategy=strategy_func,\n",
    "        encoding_method=\"naive\",\n",
    "        method_name=method_name,\n",
    "        **params\n",
    "    )\n",
    "    print(f\"     Recall@10: {results[method_name]['recall@10']:.4f}, MRR: {results[method_name]['mrr']:.4f}\")\n",
    "    \n",
    "    # Late chunking\n",
    "    print(f\"\\n  2. Late Chunking...\")\n",
    "    method_name = f\"{strategy_name}_late\"\n",
    "    results[method_name] = evaluate_method(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_path=DATASET_PATH,\n",
    "        chunking_strategy=strategy_func,\n",
    "        encoding_method=\"late_chunking\",\n",
    "        method_name=method_name,\n",
    "        **params\n",
    "    )\n",
    "    print(f\"     Recall@10: {results[method_name]['recall@10']:.4f}, MRR: {results[method_name]['mrr']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"All experiments completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d81aa",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d409cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results).T\n",
    "\n",
    "# Add columns for chunking strategy and encoding method\n",
    "df_results['chunking_strategy'] = df_results.index.str.rsplit('_', n=1).str[0]\n",
    "df_results['encoding_method'] = df_results.index.str.rsplit('_', n=1).str[1]\n",
    "\n",
    "print(\"\\n=== Late Chunking vs Naive Encoding Results ===\")\n",
    "print(df_results.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"late_chunking_results.csv\")\n",
    "print(\"\\nResults saved to late_chunking_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f7718",
   "metadata": {},
   "source": [
    "## Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66443fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPROVEMENT ANALYSIS: Late Chunking vs Naive\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for strategy in [\"sliding_window\", \"html_aware\", \"semantic_similarity\"]:\n",
    "    naive_key = f\"{strategy}_naive\"\n",
    "    late_key = f\"{strategy}_late\"\n",
    "    \n",
    "    if naive_key in results and late_key in results:\n",
    "        naive_recall = results[naive_key]['recall@10']\n",
    "        late_recall = results[late_key]['recall@10']\n",
    "        naive_mrr = results[naive_key]['mrr']\n",
    "        late_mrr = results[late_key]['mrr']\n",
    "        \n",
    "        recall_improvement = ((late_recall - naive_recall) / naive_recall * 100) if naive_recall > 0 else 0\n",
    "        mrr_improvement = ((late_mrr - naive_mrr) / naive_mrr * 100) if naive_mrr > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{strategy.upper().replace('_', ' ')}:\")\n",
    "        print(f\"  Naive:         Recall@10={naive_recall:.4f}, MRR={naive_mrr:.4f}\")\n",
    "        print(f\"  Late Chunking: Recall@10={late_recall:.4f}, MRR={late_mrr:.4f}\")\n",
    "        print(f\"  Improvement:   Recall@10: {recall_improvement:+.2f}%, MRR: {mrr_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6808ee",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "strategies = [\"Sliding Window\", \"HTML-Aware\", \"Semantic Similarity\"]\n",
    "naive_recalls = []\n",
    "late_recalls = []\n",
    "naive_mrrs = []\n",
    "late_mrrs = []\n",
    "\n",
    "for strategy in [\"sliding_window\", \"html_aware\", \"semantic_similarity\"]:\n",
    "    naive_key = f\"{strategy}_naive\"\n",
    "    late_key = f\"{strategy}_late\"\n",
    "    \n",
    "    naive_recalls.append(results[naive_key]['recall@10'])\n",
    "    late_recalls.append(results[late_key]['recall@10'])\n",
    "    naive_mrrs.append(results[naive_key]['mrr'])\n",
    "    late_mrrs.append(results[late_key]['mrr'])\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Recall@10 Comparison\n",
    "bars1 = axes[0].bar(x - width/2, naive_recalls, width, label='Naive Encoding', color='#1f77b4')\n",
    "bars2 = axes[0].bar(x + width/2, late_recalls, width, label='Late Chunking', color='#ff7f0e')\n",
    "\n",
    "axes[0].set_ylabel('Recall@10', fontsize=12)\n",
    "axes[0].set_title('Recall@10: Naive vs Late Chunking', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(strategies)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: MRR Comparison\n",
    "bars3 = axes[1].bar(x - width/2, naive_mrrs, width, label='Naive Encoding', color='#1f77b4')\n",
    "bars4 = axes[1].bar(x + width/2, late_mrrs, width, label='Late Chunking', color='#ff7f0e')\n",
    "\n",
    "axes[1].set_ylabel('MRR', fontsize=12)\n",
    "axes[1].set_title('MRR: Naive vs Late Chunking', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(strategies)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].set_ylim([0, max(max(naive_mrrs), max(late_mrrs)) * 1.2])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('late_chunking_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to late_chunking_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3700c44",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best overall method\n",
    "best_method = max(results.items(), key=lambda x: x[1]['recall@10'])\n",
    "print(f\"\\nüèÜ Best Overall Method: {best_method[0]}\")\n",
    "print(f\"   Recall@10: {best_method[1]['recall@10']:.4f}\")\n",
    "print(f\"   MRR: {best_method[1]['mrr']:.4f}\")\n",
    "\n",
    "# Compare best late chunking vs best naive\n",
    "best_naive = max([v for k, v in results.items() if 'naive' in k], key=lambda x: x['recall@10'])\n",
    "best_late = max([v for k, v in results.items() if 'late' in k], key=lambda x: x['recall@10'])\n",
    "\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "print(f\"   Best Naive Method:  Recall@10={best_naive['recall@10']:.4f}, MRR={best_naive['mrr']:.4f}\")\n",
    "print(f\"   Best Late Chunking: Recall@10={best_late['recall@10']:.4f}, MRR={best_late['mrr']:.4f}\")\n",
    "\n",
    "improvement = ((best_late['recall@10'] - best_naive['recall@10']) / best_naive['recall@10'] * 100)\n",
    "print(f\"\\nüìà Late Chunking Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLate Chunking provides contextual embeddings by encoding the full document\")\n",
    "print(\"before chunking at the token level. This preserves semantic information from\")\n",
    "print(\"surrounding text, leading to improved retrieval performance.\")\n",
    "print(\"\\nCombining the best chunking strategy (HTML-Aware) with Late Chunking\")\n",
    "print(\"encoding achieves optimal results on HTML-structured documents.\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
